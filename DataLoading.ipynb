{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c058495-4403-4bb2-89d2-c832e0eff94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7b2b81-3494-48b9-8ab6-f3493f3e1be2",
   "metadata": {},
   "source": [
    "### Extractng Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca0a25e-e8e8-4fa4-bf7e-9fadeb280a45",
   "metadata": {},
   "source": [
    "#### MFCCs\n",
    "Extract MFCC features from the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2012ba6d-8bc1-4c4e-adef-42e483d96608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mfcc(X, sample_rate, n_mfcc=42):\n",
    "    mfccs = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=n_mfcc)\n",
    "    mfccs = np.mean(mfccs.T, axis=0)  # Mean across time frames to get 1D array/vector\n",
    "    return mfccs # 1D array/vector, mean of MFCC features (shape: n_mfcc,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b586cf6d-6ad3-4a63-a870-4b74f0ee82ae",
   "metadata": {},
   "source": [
    "#### Chroma\n",
    "Extract Chroma features from the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0db7fa23-78a6-4f86-b083-d8053255fc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_chroma(X, sample_rate):\n",
    "    stft = np.abs(librosa.stft(X))  # Compute the STFT (Short-Time Fourier Transform) of the signal\n",
    "    chroma = librosa.feature.chroma_stft(S=stft, sr=sample_rate)\n",
    "    chroma = np.mean(chroma.T, axis=0)  # Mean across the time frames\n",
    "    return chroma # 1D array/vector, mean chroma features (shape: 12,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48f135-edd6-4c4a-936b-8bcdc48fd360",
   "metadata": {},
   "source": [
    "#### Mel\n",
    "Extract Mel-spectrogram features from the audio signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "99c9db51-2156-418d-b27f-a2061632d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_mel(X, sample_rate):\n",
    "    mel = librosa.feature.melspectrogram(y=X, sr=sample_rate)\n",
    "    mel = np.mean(mel.T, axis=0) # Mean across the time frames\n",
    "    return mel # 1D array/vector, mean Mel-spectrogram features (shape: 128,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff7fc28-8a6a-4b7f-adad-13de592cadb7",
   "metadata": {},
   "source": [
    "#### Combining the functions\n",
    "Extract audio features (MFCC, Chroma, Mel-spectrogram) from an audio file.\n",
    "\n",
    "Parameters:\n",
    "- file_name: str, path to the audio file.\n",
    "- mfcc: bool, whether to extract MFCC features (default True).\n",
    "- chroma: bool, whether to extract Chroma features (default True).\n",
    "- mel: bool, whether to extract Mel-spectrogram features (default True).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8992abbd-87ed-4ba1-88a7-6b3b1e8299c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name, mfcc=True, chroma=True, mel=True):\n",
    "    \n",
    "    X, sample_rate = librosa.load(os.path.join(file_name), res_type='kaiser_fast')\n",
    "    \n",
    "    result = np.array([])\n",
    "    \n",
    "    if mfcc:\n",
    "        mfccs = extract_mfcc(X, sample_rate)\n",
    "        result = np.hstack((result, mfccs))\n",
    "\n",
    "    if chroma:\n",
    "        chroma = extract_chroma(X, sample_rate)\n",
    "        result = np.hstack((result,chroma))\n",
    "\n",
    "    if mel:\n",
    "        mel = extract_mel(X, sample_rate)\n",
    "        result = np.hstack((result,mel))\n",
    "\n",
    "    return result # numpy array, concatenated feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d37ba61-ab65-4e9e-a0a8-ede8302b8be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotions present in the dataset RAVDESS and TESS\n",
    "emotions = {'01':'neutral', '02':'calm', '03':'happy', '04':'sad', '05':'angry', '06':'fearful', '07':'disgust', '08':'surprised'}\n",
    "\n",
    "# Emotions that we want to predict \n",
    "# observed_emotions = ['neutral', 'calm', 'happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']\n",
    "observed_emotions = ['happy', 'sad', 'angry', 'fearful', 'disgust', 'surprised']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c11ac78-9f8d-4c32-8c2b-bace9c7538a2",
   "metadata": {},
   "source": [
    "### Loading the Data and Feature engineering\n",
    "    Split the Dataset\n",
    "    Letâ€™s keep the test set 25% of everything and use the loadData function for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35226c83-5136-4508-aecf-f92d568970c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData(speeches = True, songs = True, tess = True, mfccs = True, chromas = False, mels = False):\n",
    "    # Initializing the feature and output labels\n",
    "    x,y =[],[]\n",
    "    \n",
    "    # Loading the data and extracting the features for each audio file (RAVDESS)\n",
    "    if speeches:\n",
    "        for file in glob.glob('Audio_Speech_Actors_01-24/Actor_*/*.wav'): \n",
    "            file_name = os.path.basename(file)\n",
    "            emotion = emotions[file_name.split(\"-\")[2]]\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "            feature = extract_features(file, mfcc=mfccs, chroma=chromas, mel=mels)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "        \n",
    "    # Loading the data and extracting the features for each song file (RAVDESS)\n",
    "    if songs:\n",
    "        for file in glob.glob('Audio_Song_Actors_01-24/Actor_*/*.wav'): \n",
    "            file_name = os.path.basename(file)\n",
    "            emotion = emotions[file_name.split(\"-\")[2]]\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "            feature = extract_features(file, mfcc=mfccs, chroma=chromas, mel=mels)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "        \n",
    "    # Loading the data and extracting the features for each audio file (TESS)\n",
    "    if tess:\n",
    "        for file in glob.glob('TESS_Toronto_emotional_speech_set_data/*/*.wav'): \n",
    "            file_name = os.path.basename(file)\n",
    "            emotion = file_name.split(\"_\")[-1]\n",
    "            emotion = emotion.split(\".\")[0]\n",
    "            if emotion == \"ps\":\n",
    "                emotion = \"surprised\"\n",
    "            if emotion == \"fear\":\n",
    "                emotion = \"fearful\"\n",
    "            if emotion not in observed_emotions:\n",
    "                continue\n",
    "            feature = extract_features(file, mfcc=mfccs, chroma=chromas, mel=mels)\n",
    "            x.append(feature)\n",
    "            y.append(emotion)\n",
    "        \n",
    "    return  {\"X\":x,\"y\":y}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b2c3f1-742b-40ff-bb83-1c62e9a3c203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smlenv",
   "language": "python",
   "name": "smlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
